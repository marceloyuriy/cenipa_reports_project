# -*- coding: utf-8 -*-
"""cenipa_tests.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fRQkZ45p-a6Dd_rBXir6TTcg2toq7VEm
"""

#Instalando as dependências no ambiente
!pip -q install docling PyPDF2 huggingface_hub

USE_DRIVE = True  # mude para False se não quiser Drive

import os
from pathlib import Path

#Montando o drive para ler e salvar os arquivos
if USE_DRIVE:
    from google.colab import drive
    drive.mount('/content/drive')

    BASE_DIR = "/content/drive/MyDrive/Mestrado/Cenipa_Prj"
else:
    BASE_DIR = "Altere aqui para o diretorio local"

#Logando no huggingface para salvar o dataset
from google.colab import userdata


INPUT_DIR   = f"{BASE_DIR}/input"
OUTPUT_DIR  = f"{BASE_DIR}/output"
DEBUG_MD_DIR= f"{BASE_DIR}/debug_md"
os.makedirs(INPUT_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(DEBUG_MD_DIR, exist_ok=True)

# -*- coding: utf-8 -*-
"""
CENIPA – extractor robusto v2 (fix MULTILINE + '## <número>' como parada)
- Regex tolerante a numeração (1, 1.1, 1.1.1) e pontuação (. : – — -)
- STOP keywords ampliadas
- Para em títulos conhecidos OU em qualquer '## <número>'
- Fallback para PyPDF2 se Docling falhar
"""

from __future__ import annotations
import os, re, json, unicodedata
from typing import Dict, Any, Optional

# ===========================
# Preparando ambiente Docling
# ===========================
DOC_AVAILABLE = True
DocumentConverter = None
DocumentConversionInput = None
try:
    from docling.document_converter import DocumentConverter as _DC
    DocumentConverter = _DC
    try:
        from docling.document_converter import DocumentConversionInput as _DCI
        DocumentConversionInput = _DCI
    except Exception:
        DocumentConversionInput = None
except Exception:
    try:
        from docling import DocumentConverter as _DC_old
        DocumentConverter = _DC_old
        from docling import DocumentConversionInput as _DCI_old
        DocumentConversionInput = _DCI_old
    except Exception:
        DOC_AVAILABLE = False
        print("Docling não disponível ou API inesperada")

converter = None
if DOC_AVAILABLE:
    try:
        converter = DocumentConverter()
    except Exception as e:
        print("Falha ao instanciar DocumentConverter:", e)
        DOC_AVAILABLE = False

# =======================
# Pré-processamento
# =======================

def preprocess_text(txt: str) -> str:
    # Normaliza Unicode; uniformiza quebras; limpa tabelas Markdown/TOC; comprime espaços
    t = unicodedata.normalize("NFKC", txt)
    t = t.replace("\r\n", "\n").replace("\r", "\n")
    # linhas tipo |----| (tabelas md)
    t = re.sub(r"\n?\s*\|[-\s|:]+\|\s*\n?", "\n", t)
    t = t.replace("|", " ")
    # linhas de pontilhado com números ao fim (índice)
    t = re.sub(r"(?m)^[^\n]*\.{3,}\s*\d+\s*$", "", t)
    # remove bloco de índice/sumário até o primeiro cabeçalho 1.x
    m_idx = re.search(r"(?is)^\s*(ÍNDICE|ÍNDICE REMISSIVO|SUMÁRIO)\b.*?(?=^\s*(?:##\s*)?1[\.:\s])", t)
    if m_idx:
        t = t[:m_idx.start()] + t[m_idx.end():]
    # colapsa quebras e espaços
    t = re.sub(r"\n{3,}", "\n\n", t)
    t = re.sub(r"[ \t]{2,}", " ", t)
    return t.strip()

# ===============================================
# Extração de texto (Docling -> PyPDF2 fallback)
# ===============================================

def _safe_get_text_from_docling_result(res) -> str:
    if hasattr(res, "text") and isinstance(res.text, str) and res.text.strip():
        return res.text
    for attr in ("markdown", "md", "markdown_str"):
        if hasattr(res, attr):
            md = getattr(res, attr)
            if isinstance(md, str) and md.strip():
                return md
    for m in ("export_to_markdown", "to_markdown", "as_markdown"):
        if hasattr(res, m):
            try:
                md = getattr(res, m)()
                if isinstance(md, str) and md.strip():
                    return md
            except Exception:
                pass
    doc = getattr(res, "document", None)
    if doc is not None:
        if hasattr(doc, "text") and isinstance(doc.text, str) and doc.text.strip():
            return doc.text
        for attr in ("markdown", "md", "markdown_str"):
            if hasattr(doc, attr):
                md = getattr(doc, attr)
                if isinstance(md, str) and md.strip():
                    return md
        for m in ("export_to_markdown", "to_markdown", "as_markdown"):
            if hasattr(doc, m):
                try:
                    md = getattr(doc, m)()
                    if isinstance(md, str) and md.strip():
                        return md
                except Exception:
                    pass
    for name in ("documents", "outputs", "pages", "results"):
        col = getattr(res, name, None)
        if isinstance(col, (list, tuple)) and col:
            for item in col:
                try:
                    txt = _safe_get_text_from_docling_result(item)
                    if isinstance(txt, str) and txt.strip():
                        return txt
                except Exception:
                    continue
    return ""


def _extract_with_pypdf2(path: str) -> str:
    try:
        from PyPDF2 import PdfReader
        reader = PdfReader(path)
        parts = []
        for page in reader.pages:
            try:
                parts.append(page.extract_text() or "")
            except Exception:
                continue
        return "\n".join(parts)
    except Exception:
        return ""


def extrair_texto(caminho_pdf: str) -> str:
    if DOC_AVAILABLE and converter is not None:
        try:
            if DocumentConversionInput is not None:
                conv_input = DocumentConversionInput.from_paths([caminho_pdf])
                results = converter.convert(conv_input)
                base = results[0] if isinstance(results, list) and results else results
                txt = _safe_get_text_from_docling_result(base)
            else:
                txt = _safe_get_text_from_docling_result(converter.convert(caminho_pdf))
            if isinstance(txt, str) and txt.strip():
                return txt
        except Exception as e:
            print(f"[Docling] Erro em {os.path.basename(caminho_pdf)}: {e}")
    # Fallback
    txt = _extract_with_pypdf2(caminho_pdf)
    return txt

# ==============================================================
# Lógica de Extração de Seções
# ==============================================================

# Cabeçalho base (início de linha, possível '##', e numeração 1 / 1.1 / 1.1.1, com ou sem ponto e espaço)
BASE  = r"^\s*(?:#{1,6}\s*)?(?:\d+(?:\.\d+)*\s*[.:–—-]?\s*)?"
# Pontuação que pode seguir o título
PUNCT = r"\s*[.:–—-]?\s*"

# Palavras-chave (variações e sinônimos)
KEYWORDS = {
    "historico_do_voo": [r"HIST[ÓO]RICO\s+DO\s+VOO"],
    "analise":          [r"AN[ÁA]LISE", r"COMENT[ÁA]RIOS"],
    "fatos":            [r"FATOS"],
    "fatores_contribuintes": [r"FATORES\s+CONTRIBUINTES"],
}

# Títulos que encerram uma seção (inclui o próprio conjunto de inícios)
ALL_STOP_KEYWORDS = [
    r"SINOPSE",
    r"GLOSS[ÁA]RIO",
    r"INFORMA[ÇC][ÕO]ES\s+FACTUAIS",
    r"HIST[ÓO]RICO\s+DO\s+VOO",
    r"COMENT[ÁA]RIOS",
    r"AN[ÁA]LISE",
    r"FATOS",
    r"FATORES\s+CONTRIBUINTES",
    r"CONCLUS[ÕO]ES?",
    r"RECOMENDA[ÇC][ÕO]ES(?:\s+DE\s+SEGURAN[ÇC]A)?",
    r"A[ÇC][ÕO]ES\s+CORRETIVAS(?:\s+OU\s+PREVENTIVAS\s+ADOTADAS)?",
]

# Compila padrões (MULTILINE para que ^/$ funcionem por linha)
_re_flags = re.IGNORECASE | re.UNICODE | re.MULTILINE

PADROES_INICIO = {
    k: re.compile(BASE + r"(" + "|".join(v) + r")\b" + PUNCT, _re_flags)
    for k, v in KEYWORDS.items()
}

# Para parar: (a) próximo título conhecido OU (b) qualquer cabeçalho '##' numérico
PADRAO_PARADA = re.compile(
    BASE + r"(" + "|".join(ALL_STOP_KEYWORDS) + r")\b" + PUNCT
    + r"|^\s*##\s*\d+(?:\.\d+)*\b.*$",
    _re_flags,
)

def extrair_secao(texto: str, padrao_inicio: re.Pattern, padrao_parada: re.Pattern) -> str:
    """
    Extrai conteúdo entre um título de início e o próximo título de parada.
    Para de forma segura em qualquer '## <número>' subsequente.
    """
    try:
        m_ini = padrao_inicio.search(texto)
        if not m_ini:
            return ""

        trecho = texto[m_ini.end():]

        # Próximo título conhecido OU qualquer '## 1.x'
        m_next = padrao_parada.search(trecho)
        fim = m_next.start() if m_next else len(trecho)
        bruto = trecho[:fim]

        # Guarda extra: se ainda sobrar algum '## <número>' dentro do bloco, corta ali
        m_fallback = re.search(r"^\s*##\s*\d+(?:\.\d+)*\b.*$", bruto, re.MULTILINE)
        if m_fallback:
            bruto = bruto[:m_fallback.start()]

        # Normaliza
        bruto = bruto.replace("\r", "\n")
        bruto = re.sub(r"\n{2,}", "\n", bruto)
        bruto = bruto.strip()

        bruto = bruto.replace("\n", " ")
        bruto = re.sub(r"-\s{1,2}", "", bruto)   # hifenização de quebra de linha
        bruto = re.sub(r"\s{2,}", " ", bruto)
        return bruto.strip()
    except Exception as e:
        print(f"Erro ao extrair seção: {e}")
        return ""

# ===============================================
# Execução sobre diretório
# ===============================================

def processar_relatorios(diretorio: str, dump_md_dir: Optional[str] = None) -> Dict[str, Any]:
    dados_compilados: Dict[str, Any] = {}
    for nome_arquivo in sorted(os.listdir(diretorio)):
        if not nome_arquivo.lower().endswith(".pdf"):
            continue
        caminho = os.path.join(diretorio, nome_arquivo)
        print(f"Processando: {nome_arquivo}")
        texto_relatorio = extrair_texto(caminho)
        if not texto_relatorio:
            print(f"   ⚠️ Falha na extração: {nome_arquivo}")
            continue
        texto_relatorio = preprocess_text(texto_relatorio)
        if dump_md_dir:
            os.makedirs(dump_md_dir, exist_ok=True)
            base = os.path.splitext(nome_arquivo)[0]
            with open(os.path.join(dump_md_dir, f"{base}.txt"), "w", encoding="utf-8") as f:
                f.write(texto_relatorio)
        ident = os.path.splitext(nome_arquivo)[0].replace('.', '_')
        dados_compilados[ident] = {
            "arquivo_origem": nome_arquivo
        }
        for secao, patt in PADROES_INICIO.items():
            dados_compilados[ident][secao] = extrair_secao(texto_relatorio, patt, PADRAO_PARADA)
    return dados_compilados


def salvar_json(dados: Dict[str, Any], nome_arquivo_saida: str) -> None:
    with open(nome_arquivo_saida, 'w', encoding='utf-8') as f:
        json.dump(dados, f, ensure_ascii=False, indent=2)
    print(f"\n✅ Dados extraídos foram salvos em '{nome_arquivo_saida}'")


def run_pipeline(input_dir: str, output_dir: str, debug_dir: Optional[str] = None, saida_nome: str = "extracao_relatorios_final.json") -> Optional[str]:
    dados = processar_relatorios(input_dir, dump_md_dir=debug_dir)
    if not dados:
        print("Nenhum PDF válido processado.")
        return None
    os.makedirs(output_dir, exist_ok=True)
    saida = os.path.join(output_dir, saida_nome)
    salvar_json(dados, saida)
    return saida

# ====== utilitário opcional de debug ======
def listar_cabecalhos(texto: str) -> list[str]:
    """Retorna todos os títulos (incluindo '## <número>') que o PADRAO_PARADA enxerga."""
    return [m.group(0).strip() for m in PADRAO_PARADA.finditer(texto)]

saida_json = run_pipeline(INPUT_DIR, OUTPUT_DIR, debug_dir=DEBUG_MD_DIR)
print("Saída JSON:", saida_json)